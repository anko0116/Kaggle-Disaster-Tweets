{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\nimport requests\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text as text\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import StratifiedShuffleSplit\nimport os\n\n# Super useful RNN guide: https://stackoverflow.com/questions/48714407/rnn-regularization-which-component-to-regularize\n# Code below is modified from https://www.kaggle.com/fmitchell259/disaster-tweets-naive-bayes-svm-rnn\nEMOJIS = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', \n          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed', \n          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink', \n          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}\n\ndef remove_punct(tweet):\n    return re.sub(r'[^\\w\\s]', '', tweet)\ndef remove_word(word):\n    def remove_specific(tweet):\n        return tweet.replace(word, '')\n    return remove_specific\ndef remove_url(string):\n    return re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%|\\-)*\\b', '', string)\ndef remove_html(string):\n    return re.sub(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});', '', string)\ndef remove_useless_char(string):\n    thestring = re.sub(r'[^a-zA-Z\\s]','', string)\n    thestring = re.sub(r'\\b\\w{1,2}\\b', '', thestring)\n    return re.sub(' +', ' ', thestring) \ndef remove_emojis(string):\n    for emoji in EMOJIS.keys():\n        string = string.replace(emoji, \"EMOJI\" + EMOJIS[emoji])  \n    return string\ndef make_lowercase(string):\n    return string.lower()\ndef remove_nums(string):\n    return re.sub(r'\\d+', '', string)\n\ndef preprocess(dataset):\n    # Helpful link for tokenization\n    # https://www.kdnuggets.com/2020/03/tensorflow-keras-tokenization-text-data-prep.html\n    dataset[\"text\"] = dataset[\"text\"].apply(make_lowercase)\n    dataset[\"text\"] = dataset[\"text\"].apply(remove_url)\n    dataset[\"text\"] = dataset[\"text\"].apply(remove_html)\n    dataset[\"text\"] = dataset[\"text\"].apply(remove_punct)\n    dataset[\"text\"] = dataset[\"text\"].apply(remove_nums)\n    dataset[\"text\"] = dataset[\"text\"].apply(remove_emojis)\n    return dataset        ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-27T06:25:36.498884Z","iopub.execute_input":"2021-12-27T06:25:36.499377Z","iopub.status.idle":"2021-12-27T06:25:40.606516Z","shell.execute_reply.started":"2021-12-27T06:25:36.499326Z","shell.execute_reply":"2021-12-27T06:25:40.605618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load and preprcoess train dataset and test dataset\ntrain_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\ntrain_df = train_df.drop(columns=[\"id\", \"keyword\", \"location\"])\ntest_df = test_df.drop(columns=[\"keyword\", \"location\"])\ntest_df['text'] = test_df['text'].replace(\"\", \"empty\") # For tweets with empty strings\n\ntrain_df = preprocess(train_df)\ntest_df = preprocess(test_df)","metadata":{"execution":{"iopub.status.busy":"2021-12-27T06:03:16.339723Z","iopub.execute_input":"2021-12-27T06:03:16.339944Z","iopub.status.idle":"2021-12-27T06:03:17.058113Z","shell.execute_reply.started":"2021-12-27T06:03:16.339917Z","shell.execute_reply":"2021-12-27T06:03:17.057285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build token vocabulary\ntokenizer = Tokenizer(oov_token=\"<UNK>\")\ntokenizer.fit_on_texts(train_df[\"text\"])\n\n# Tokenize\ntrain_text = tokenizer.texts_to_sequences(train_df[\"text\"])\ntest_text = tokenizer.texts_to_sequences(test_df[\"text\"])\n\n# Get max training sequence length\nmaxlen = max([len(x) for x in train_text])\n\n# Pad the training sequences\npad_type = 'post'\ntrunc_type = 'post'\ntrain_text = pad_sequences(train_text, padding=pad_type, truncating=trunc_type, maxlen=maxlen)\ntest_text = pad_sequences(test_text, padding=pad_type, truncating=trunc_type, maxlen=maxlen)\n\n# Convert to TensorFlow Dataset\ntarget = train_df.pop('target') # Get only target/label values\ntarget = tf.reshape(target, shape=(7613, 1))\ntrain_text = tf.reshape(train_text, shape=(7613, 1, 23))\ntest_text = tf.reshape(test_text, shape=(3263, 1, 23))\ntrain_tf = tf.data.Dataset.from_tensor_slices((train_text, target))\ntest_tf = tf.data.Dataset.from_tensor_slices((test_text))\n\n# Split the training data into train and valid\ntrain_tf = train_tf.shuffle(7613, reshuffle_each_iteration=False)\ntrain_tf_valid = train_tf.take(2283)\ntrain_tf_train = train_tf.skip(2283)","metadata":{"execution":{"iopub.status.busy":"2021-12-27T06:03:17.059406Z","iopub.execute_input":"2021-12-27T06:03:17.059645Z","iopub.status.idle":"2021-12-27T06:03:17.362851Z","shell.execute_reply.started":"2021-12-27T06:03:17.059617Z","shell.execute_reply":"2021-12-27T06:03:17.362185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model code\nvocab_size = len(tokenizer.word_index) + 2\ntf.random.set_seed(42)\nembedding_dimension = 100\n\nbi_lstm_model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dimension),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(100, activation='relu')),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nbi_lstm_model.compile(optimizer=tf.keras.optimizers.Adam(5e-4),\n                     loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n                     metrics=['accuracy'])\n\nhistory = bi_lstm_model.fit(train_tf_train, validation_data=train_tf_valid, epochs=5, callbacks=[tf.keras.callbacks.EarlyStopping(patience=2)])\nprint(\"TRAINING DONE\")","metadata":{"execution":{"iopub.status.busy":"2021-12-26T11:35:39.189721Z","iopub.execute_input":"2021-12-26T11:35:39.19026Z","iopub.status.idle":"2021-12-26T11:46:22.162123Z","shell.execute_reply.started":"2021-12-26T11:35:39.190216Z","shell.execute_reply":"2021-12-26T11:46:22.161274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = bi_lstm_model.predict(test_tf, batch_size=None)\npredictions = np.round(predictions).astype(np.short)\n\noutput_df = pd.DataFrame(test_df['id'])\noutput_df['target'] = predictions\n\nout = output_df.to_csv('hyukahn_rnn.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-26T11:46:22.163783Z","iopub.execute_input":"2021-12-26T11:46:22.164147Z","iopub.status.idle":"2021-12-26T11:46:39.763984Z","shell.execute_reply.started":"2021-12-26T11:46:22.164106Z","shell.execute_reply":"2021-12-26T11:46:39.762982Z"},"trusted":true},"execution_count":null,"outputs":[]}]}